# -*- coding: utf-8 -*-
"""Resume Screening with Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Qi9MasAb2stqQs1ZCSHJKRZKI_x4Xzk
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/SEM_6/Hacknitr/Dataset2.csv')

df.head()

df.shape

df['Resume'][0]

"""# Exploring Categories"""

df['Category'].value_counts()

plt.figure(figsize=(15,5))
sns.countplot(df['Category'])
plt.xticks(rotation=90)
plt.show()

df['Resume'].unique()

counts = df['Category'].value_counts()
labels = df['Category'].unique()
plt.figure(figsize=(15,10))

plt.pie(counts,labels=labels,autopct='%1.1f%%',shadow=True, colors=plt.cm.plasma(np.linspace(0,1,3)))
plt.show()

"""# Exploring Resume"""

df.head()

df['Category'][0]

df['Resume'][0]

"""# Cleaning Data:                                      
1 URLs,                                                
2 hashtags,                                             
3 mentions,                                                     
4 special letters,                                             
5 punctuations:                                             
"""

import re
def cleanResume(txt):
    cleanText = re.sub('http\S+\s', ' ', txt)
    cleanText = re.sub('RT|cc', ' ', cleanText)
    cleanText = re.sub('#\S+\s', ' ', cleanText)
    cleanText = re.sub('@\S+', '  ', cleanText)
    cleanText = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', cleanText)
    cleanText = re.sub(r'[^\x00-\x7f]', ' ', cleanText)
    cleanText = re.sub('\s+', ' ', cleanText)
    return cleanText

cleanResume("my #### $ #  #noorsaeed webiste like is this http://heloword and access it @gmain.com")

df['Resume'] = df['Resume'].apply(lambda x: cleanResume(x))

df['Resume'][0]

# df.to_csv('my_data.csv', index=False, encoding='utf-8', sep=';', na_rep='N/A')

import re

# Define keywords lists for various tech areas
security_keywords = [
    "endpoint", "network", "cloud", "application", "security",
    "SIEM", "firewall", "IDS", "IPS", "UBA", "EDR"
]

data_science_keywords = [
    "hadoop", "spark", "python", "r", "machine learning", "deep learning",
    "big data", "cloud", "data visualization", "business intelligence"
]

automation_keywords = [
    "rpa", "automation", "process automation", "AI", "devops"
]

metaverse_keywords = [
    "metaverse", "3d", "virtual world", "VR", "AR", "blockchain"
]

ai_ml_keywords = [
    "artificial intelligence", "machine learning", "deep learning",
    "computer vision", "natural language processing", "chatbot"
]

nlp_keywords = [
    "natural language processing", "chatbot", "sentiment analysis",
    "text summarization", "language understanding"
]

generative_ai_keywords = [
    "generative AI", "AI-powered", "content generation", "personalization"
]

# Combine keyword lists for broader techstack coverage
all_keywords = security_keywords + data_science_keywords + automation_keywords + \
                metaverse_keywords + ai_ml_keywords + nlp_keywords + generative_ai_keywords

# Define the regular expression with word boundaries and case-insensitive flag
techstack_regex = r"\b(" + "|".join(all_keywords) + r")\b"  # Join keywords with OR operator

def find_techstack(text):
  """
  This function takes text as input and returns a list of potential techstack keywords found using the regex.
  """
  matches = re.findall(techstack_regex, text, flags=re.IGNORECASE)
  return matches

# Example usage
# text = "We use Wipro for endpoint security, cloud security, and network security. We also leverage RPA for process automation."
techstack_terms = find_techstack(df['Resume'][0])
print(techstack_terms)  # Output: ['endpoint security', 'cloud security', 'network security', 'RPA']

"""# words into categorical values"""

from sklearn.preprocessing import LabelEncoder
le1 = LabelEncoder()

le1.fit(df['Category'])
df['Category'] = le1.transform(df['Category'])

df['Category']

le = LabelEncoder()
le.fit(df['Resume'])
df['Resume'] = le.transform(df['Resume'])

df['Resume']

df.Category.unique()

df.Resume.unique()

# ['Data Science', 'HR', 'Advocate', 'Arts', 'Web Designing',
#        'Mechanical Engineer', 'Sales', 'Health and fitness',
#        'Civil Engineer', 'Java Developer', 'Business Analyst',
#        'SAP Developer', 'Automation Testing', 'Electrical Engineering',
#        'Operations Manager', 'Python Developer', 'DevOps Engineer',
#        'Network Security Engineer', 'PMO', 'Database', 'Hadoop',
#        'ETL Developer', 'DotNet Developer', 'Blockchain', 'Testing'],
#       dtype=object)

df['Category']

df['Resume']

"""# Vactorization"""

# from sklearn.feature_extraction.text import TfidfVectorizer
# # tfidf = TfidfVectorizer(stop_words='english')

# tfidf.fit(techstack_terms)
# requredTaxt  = tfidf.transform(techstack_terms)

# print(requredTaxt)

"""# Splitting"""

df['Category']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df['Resume'], df['Category'], test_size=0.2, random_state=42)

df['Category'].shape

X_train

X_train.shape

X_test.shape

"""# Now letâ€™s train the model and print the classification report:"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score

clf = OneVsRestClassifier(KNeighborsClassifier())
clf.fit(X_train,y_train)
ypred = clf.predict(X_test)
print(accuracy_score(y_test,ypred))

ypred

"""# Prediction System"""

import pickle
pickle.dump(tfidfd,open('tfidf.pkl','wb'))
pickle.dump(clf, open('clf.pkl', 'wb'))

myresume = """I am a data scientist specializing in machine
learning, deep learning, and computer vision. With
a strong background in mathematics, statistics,
and programming, I am passionate about
uncovering hidden patterns and insights in data.
I have extensive experience in developing
predictive models, implementing deep learning
algorithms, and designing computer vision
systems. My technical skills include proficiency in
Python, Sklearn, TensorFlow, and PyTorch.
What sets me apart is my ability to effectively
communicate complex concepts to diverse
audiences. I excel in translating technical insights
into actionable recommendations that drive
informed decision-making.
If you're looking for a dedicated and versatile data
scientist to collaborate on impactful projects, I am
eager to contribute my expertise. Let's harness the
power of data together to unlock new possibilities
and shape a better future.
Contact & Sources
Email: 611noorsaeed@gmail.com
Phone: 03442826192
Github: https://github.com/611noorsaeed
Linkdin: https://www.linkedin.com/in/noor-saeed654a23263/
Blogs: https://medium.com/@611noorsaeed
Youtube: Artificial Intelligence
ABOUT ME
WORK EXPERIENCE
SKILLES
NOOR SAEED
LANGUAGES
English
Urdu
Hindi
I am a versatile data scientist with expertise in a wide
range of projects, including machine learning,
recommendation systems, deep learning, and computer
vision. Throughout my career, I have successfully
developed and deployed various machine learning models
to solve complex problems and drive data-driven
decision-making
Machine Learnine
Deep Learning
Computer Vision
Recommendation Systems
Data Visualization
Programming Languages (Python, SQL)
Data Preprocessing and Feature Engineering
Model Evaluation and Deployment
Statistical Analysis
Communication and Collaboration
"""

import pickle

# Load the trained classifier
clf = pickle.load(open('clf.pkl', 'rb'))

# Clean the input resume
cleaned_resume = cleanResume(myresume)

# Transform the cleaned resume using the trained TfidfVectorizer
input_features = tfidfd.transform([cleaned_resume])

# Make the prediction using the loaded classifier
prediction_id = clf.predict(input_features)[0]

# Map category ID to category name
category_mapping = {
    15: "Java Developer",
    23: "Testing",
    8: "DevOps Engineer",
    20: "Python Developer",
    24: "Web Designing",
    12: "HR",
    13: "Hadoop",
    3: "Blockchain",
    10: "ETL Developer",
    18: "Operations Manager",
    6: "Data Science",
    22: "Sales",
    16: "Mechanical Engineer",
    1: "Arts",
    7: "Database",
    11: "Electrical Engineering",
    14: "Health and fitness",
    19: "PMO",
    4: "Business Analyst",
    9: "DotNet Developer",
    2: "Automation Testing",
    17: "Network Security Engineer",
    21: "SAP Developer",
    5: "Civil Engineer",
    0: "Advocate",
}

category_name = category_mapping.get(prediction_id, "Unknown")

print("Predicted Category:", category_name)
print(prediction_id)

